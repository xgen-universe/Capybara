import os
import torch
import imageio
from PIL import Image


############################
# Resolution and Bucket Functions
############################

def get_aspect_ratio_from_bucket(bucket):
    """
    Determine aspect_ratio based on bucket.

    Args:
        bucket: Bucket in (H, W) format

    Returns:
        str: aspect_ratio string
    """
    bucket_h, bucket_w = bucket

    # Determine aspect_ratio based on bucket's width-to-height ratio
    if bucket == (544, 720):
        return "4:3"
    elif bucket == (720, 544):
        return "3:4"
    elif bucket == (640, 640):
        return "1:1"
    elif bucket == (480, 848):
        return "9:16"
    elif bucket == (848, 480):
        return "16:9"
    else:
        # For other buckets, infer based on aspect ratio
        ratio = bucket_w / bucket_h
        if abs(ratio - 16/9) < abs(ratio - 9/16):
            return "16:9"
        elif abs(ratio - 9/16) < abs(ratio - 1.0):
            return "9:16"
        elif abs(ratio - 4/3) < abs(ratio - 3/4):
            return "4:3"
        elif abs(ratio - 3/4) < abs(ratio - 1.0):
            return "3:4"
        else:
            return "1:1"


def get_bucket_from_resolution_and_aspect_ratio(resolution, aspect_ratio):
    """
    Get the corresponding bucket based on resolution and aspect_ratio.

    Args:
        resolution: Resolution string, options: "480p", "720p", "1024", "1080p"
        aspect_ratio: Aspect ratio string, options: "16:9", "9:16", "4:3", "3:4", "1:1"

    Returns:
        tuple: Bucket in (bucket_h, bucket_w) format
    """
    # Define bucket groups: (H, W)
    bucket_groups = [
        # Group 1: ~400k pixels (480p)
        [(480, 848), (544, 720), (640, 640), (720, 544), (848, 480)],
        # Group 2: ~920k pixels (720p)
        [(720, 1280), (832, 1104), (960, 960), (1104, 832), (1280, 720)],
        # Group 3: ~1M pixels (1024)
        [(768, 1360), (880, 1184), (1024, 1024), (1184, 880), (1360, 768)],
        # Group 4: ~2M pixels (1080p)
        [(1088, 1920), (1248, 1664), (1440, 1440), (1664, 1248), (1920, 1088)]
    ]

    # Map resolution to group index
    resolution_to_group = {
        "480p": 0,
        "720p": 1,
        "1024": 2,
        "1080p": 3
    }

    # Map aspect_ratio to bucket index (position within the group)
    aspect_ratio_to_bucket_idx = {
        "16:9": 0,   # landscape wide
        "4:3": 1,    # landscape
        "1:1": 2,    # square
        "3:4": 3,    # portrait
        "9:16": 4    # portrait tall
    }

    if resolution not in resolution_to_group:
        raise ValueError(f"Invalid resolution: {resolution}. Must be one of: 480p, 720p, 1024, 1080p")
    if aspect_ratio not in aspect_ratio_to_bucket_idx:
        raise ValueError(f"Invalid aspect_ratio: {aspect_ratio}. Must be one of: 16:9, 9:16, 4:3, 3:4, 1:1")

    group_idx = resolution_to_group[resolution]
    bucket_idx = aspect_ratio_to_bucket_idx[aspect_ratio]

    bucket = bucket_groups[group_idx][bucket_idx]
    return bucket


def get_bucket_from_resolution_and_actual_ratio(resolution, actual_ratio):
    """
    Get the corresponding bucket based on resolution and actual aspect ratio (numeric).
    Finds the bucket with the closest aspect ratio within the group for the given resolution.

    Args:
        resolution: Resolution string, options: "480p", "720p", "1024", "1080p"
        actual_ratio: Actual aspect ratio (width/height, float)

    Returns:
        tuple: Bucket in (bucket_h, bucket_w) format
    """
    # Define bucket groups: (H, W)
    bucket_groups = [
        # Group 1: ~400k pixels (480p)
        [(480, 848), (544, 720), (640, 640), (720, 544), (848, 480)],
        # Group 2: ~920k pixels (720p)
        [(720, 1280), (832, 1104), (960, 960), (1104, 832), (1280, 720)],
        # Group 3: ~1M pixels (1024)
        [(768, 1360), (880, 1184), (1024, 1024), (1184, 880), (1360, 768)],
        # Group 4: ~2M pixels (1080p)
        [(1088, 1920), (1248, 1664), (1440, 1440), (1664, 1248), (1920, 1088)]
    ]

    # Map resolution to group index
    resolution_to_group = {
        "480p": 0,
        "720p": 1,
        "1024": 2,
        "1080p": 3
    }

    if resolution not in resolution_to_group:
        raise ValueError(f"Invalid resolution: {resolution}. Must be one of: 480p, 720p, 1024, 1080p")

    group_idx = resolution_to_group[resolution]
    group = bucket_groups[group_idx]

    # Find the bucket with the closest aspect ratio within this group
    best_bucket = None
    min_ratio_diff = float('inf')

    for bucket_h, bucket_w in group:
        bucket_ratio = bucket_w / bucket_h
        diff = abs(actual_ratio - bucket_ratio)

        if diff < min_ratio_diff:
            min_ratio_diff = diff
            best_bucket = (bucket_h, bucket_w)

    return best_bucket


def get_resolution(args, row, extra_kwargs):
    """
    Get resolution and aspect ratio based on task type and arguments.

    Args:
        args: Argument object
        row: CSV data row
        extra_kwargs: Extra kwargs dict

    Returns:
        tuple: (bucket, aspect_ratio)
    """
    # For t2v or t2i tasks, if resolution is provided
    if args.task_type in ["t2v", "t2i"]:
        if args.resolution is not None and args.aspect_ratio is not None:
            # Both resolution and aspect_ratio are given, use them directly
            bucket = get_bucket_from_resolution_and_aspect_ratio(args.resolution, args.aspect_ratio)
            aspect_ratio = args.aspect_ratio
            return bucket, aspect_ratio
        elif args.resolution is not None:
            # Only resolution is given, aspect_ratio is missing
            # For t2v/t2i tasks, there is no reference video/image to infer aspect ratio, so aspect_ratio must be provided
            raise ValueError(f"For {args.task_type} task, --aspect_ratio must be provided when --resolution is specified")
        elif args.aspect_ratio is not None:
            # Only aspect_ratio is given, resolution is missing
            raise ValueError(f"For {args.task_type} task, both --resolution and --aspect_ratio must be provided together")

    # For other task types, if resolution is given but aspect_ratio is not, infer from video/image aspect ratio
    if args.resolution is not None and args.aspect_ratio is None:
        # Get actual aspect ratio based on task type
        if args.task_type == "ti2i" or args.task_type == "i2v":
            # Get aspect ratio from image
            reference_img = extra_kwargs.get("reference_img")
            if reference_img is None:
                raise ValueError(f"Reference image is not provided for task_type: {args.task_type}")
            if not os.path.exists(reference_img):
                raise ValueError(f"Reference image file not found: {reference_img}")

            img = Image.open(reference_img)
            w, h = img.size  # PIL returns (width, height)
            if h == 0 or w == 0:
                raise ValueError(f"Image height or width is 0: {reference_img}")
            actual_ratio = w / h

            # Get bucket based on resolution and actual aspect ratio
            bucket = get_bucket_from_resolution_and_actual_ratio(args.resolution, actual_ratio)
            aspect_ratio = get_aspect_ratio_from_bucket(bucket)
        else:
            # Get aspect ratio from video
            reference_video = extra_kwargs.get("reference_video")
            if reference_video is None:
                raise ValueError(f"Reference video is not provided for task_type: {args.task_type}")
            if not os.path.exists(reference_video):
                raise ValueError(f"Reference video file not found: {reference_video}")

            # Read the first frame of the video using imageio
            try:
                reader = imageio.get_reader(reference_video)
                if reader.count_frames() == 0:
                    reader.close()
                    raise ValueError(f"Video has no frames: {reference_video}")

                first_frame = reader.get_data(0)  # Get first frame, shape: (H, W, C)
                reader.close()

                h, w = first_frame.shape[:2]  # imageio returns (height, width)
            except Exception as e:
                raise ValueError(f"Failed to read video: {reference_video}, error: {str(e)}")

            if h == 0 or w == 0:
                raise ValueError(f"Video height or width is 0: {reference_video}")
            actual_ratio = w / h

            # Get bucket based on resolution and actual aspect ratio
            bucket = get_bucket_from_resolution_and_actual_ratio(args.resolution, actual_ratio)
            aspect_ratio = get_aspect_ratio_from_bucket(bucket)

        return bucket, aspect_ratio


############################
# Instruction Rewrite
############################

INSTRUCTION_REWRITE_SYSTEM_PROMPT = """
You are a world-class video/image editing instruction expansion expert named "Hunyuan Edit Rewriter 1.5". Your core mission is to expand a short editing instruction into a detailed, **structured, objective, and thorough** editing instruction based on the provided visual content (video frame or image). The final instruction will follow a strict logical order to guide an AI model to perform precise, high-quality edits.

## I. Core Universal Principles

### A. Overall Instruction Structure
The expanded instruction follows a logical, hierarchical flow:

1. **Subject & Scene Identification:** Begin by identifying the main subject(s) and scene context from the reference visual.
   - *Example:* "In the video, a young woman in a red dress is standing on a cobblestone street with vintage European-style buildings in the background."

2. **Edit Target Specification:** Clearly state what element(s) should be edited and how.
   - *Example:* "Change the color of her dress from red to deep navy blue, maintaining the fabric texture, folds, and lighting reflections."

3. **Detail Preservation Requirements:** Specify what should remain unchanged.
   - *Example:* "The woman's pose, facial expression, hairstyle, the surrounding architecture, and the overall lighting should remain completely unchanged."

4. **Expected Visual Effect:** Describe the desired final result in detail, including specific visual changes (color, texture, shape, style, etc.).
   - *Example:* "The navy blue dress should appear natural under the warm afternoon sunlight, with subtle highlights on the fabric folds and consistent shadow patterns matching the original scene."

5. **Technical & Style Notes:** If applicable, include technical aspects like maintaining consistency, temporal coherence (for video), or style specifications.
   - *Example:* "Ensure temporal consistency across all frames, with no flickering or color inconsistencies in the edited dress."

### B. Core Grammar Rules

1. **Tense:** Use **imperative mood** or **present tense** for editing actions. This keeps instructions clear and direct.
   - *Example:* "Change the background to...", "The sky transforms into...", "Replace the object with..."

2. **Sentence Structure:**
   - Sentences should be **declarative and action-oriented**.
   - Structure: `[Edit Action] + [Target Element] + [Specific Details]`
   - Use **prepositional phrases** for spatial/visual context (e.g., "in the upper-left corner", "on the subject's face").

3. **Vocabulary and Tone:**
   - **Adjectives:** Use precise descriptive adjectives for visual attributes (e.g., "warm-toned", "semi-transparent", "matte", "glossy").
   - **Verbs:** Use specific editing action verbs (e.g., "replace", "transform", "adjust", "overlay", "remove", "enhance").
   - **Tone:** Objective and factual. Describe the visual changes concretely, not abstractly.

---

## II. Edit-Type Specific Rules

### A. Color / Style Edits
- Describe the target color or style change with precision (e.g., color hex values, material names, artistic style references).
- Reference how lighting in the original scene should interact with the new color/style.
- *Example:* "Change the car's paint color to metallic emerald green with visible reflection of the surrounding trees on its glossy surface."

### B. Object Addition / Removal / Replacement
- Clearly identify the object by its visual location and appearance in the reference.
- For additions: describe the placement, size, orientation, and how it integrates with the existing scene (shadows, reflections, occlusion).
- For removals: describe what should fill the gap (background continuation, inpainting).
- For replacements: describe both what is removed and what replaces it, ensuring visual coherence.

### C. Background / Environment Edits
- Describe the target background in detail (elements, depth, lighting, atmosphere).
- Specify how the foreground subjects should be preserved and blended with the new background.
- Address lighting consistency between foreground and new background.

### D. Video-Specific Edits (TV2V, IV2V)
- Emphasize **temporal consistency**: the edit must be coherent across all frames.
- Describe how the edit should behave over time if there is motion (e.g., "as the person walks, the changed outfit should move naturally with proper cloth dynamics").
- Specify frame-to-frame smoothness requirements.

---

## III. Standard Expansion Process

When expanding an editing instruction, follow these steps:

1. **Analyze the Reference Visual:**
   - Identify the main subject(s), their attributes (clothing, pose, expression, color).
   - Identify the background, lighting conditions, and overall scene context.
   - Note any important secondary elements.

2. **Parse the Original Instruction:**
   - Extract the core editing intent (what needs to change).
   - Identify the scope of the edit (local vs. global, single element vs. multiple).

3. **Expand with Visual Grounding:**
   - Reference specific visual elements from the input to make the instruction concrete.
   - Add details about expected visual outcomes based on the scene's lighting, perspective, and composition.

4. **Review and Validate:**
   - **Faithfulness Check:** Ensure the expanded instruction preserves the original intent exactly. Do NOT add new editing operations not implied by the original.
   - **Specificity Check:** Ensure all visual references are concrete and unambiguous.
   - **Feasibility Check:** Ensure the described edit is physically and visually plausible given the input content.
   - **Completeness Check:** Ensure all aspects of the edit are covered (what changes, what stays, expected result).

---

## IV. Output Requirements

1. **Output ONLY the expanded editing instruction.** No thinking process, no explanation, no markdown formatting.
2. **Output Language:** Match the original instruction language (English input -> English output; Chinese input -> Chinese output).
3. **Faithfulness to Input:** Preserve the core editing intent, target elements, and action type from the original instruction.
4. **Moderate Length:** Expand to approximately 2-4x the original instruction length. Be detailed but not verbose.
5. **Instruction Format:** Keep the output as an editing instruction (imperative/action-oriented). Do NOT turn it into a scene description or generation prompt.
6. **Avoid Self-Reference:** Do not start with "This instruction asks to..." or similar meta-phrases. Directly state the editing action.
"""

INSTRUCTION_REWRITE_TEXT_ONLY_SYSTEM_PROMPT = """你是世界级的视频生成提示词撰写专家，你的名字是"Hunyuan Video Rewriter 1.5"。你的核心使命是将用户提供的简单句子扩展为详细、**结构化、客观且详尽的**视频生成提示词。最终的提示词将遵循严格的逻辑顺序，从一般到具体，使用精确的专业词汇来指导AI模型生成物理逻辑合理、构图精美的高质量视频。

## **一、核心通用原则**

在构建任何提示词时，你必须遵守以下基本原则：

### I. 通用句子结构和语法规则（适用于所有视频类型）

这些规则构成了描述任何视频的基础结构，无论其风格如何。描述始终以客观、详细且易于解析的方式组织。

**A. 整体描述结构**
描述遵循逻辑和层次化的流程，从一般到具体。

1.  **主体与场景介绍：**描述几乎总是以介绍主要对象和直接场景开始。
    *   *示例：*"一只棕色皮肤的巨大霸王龙正在穿越广阔的荒漠平原。"
    *   *示例：*"在一个灯光昏暗、以砖墙为背景的舞台上，一群年轻的东亚表演者正在表演同步舞蹈。"

2.  **主体细节描述：**然后提供关于主体外观、服装和显著特征的具体细节。
    *   *示例：*"她穿着一件华丽的红金刺绣上衣，一条带有银色花卉图案的深蓝色连衣裙，以及一条配套的红金项圈。"

3.  **按时间顺序的动作序列：**动作按其发生的顺序进行描述。这一叙事部分使用过渡词，如**"最初，""然后，""接下来，""随着，"**和**"最后，"**来引导读者了解事件的顺序，注意这一部分需要详细描述，用来指导视频生成模型。
    *   *示例：*"**最初，**她睁大眼睛看向左边。**然后**镜头绕着她旋转。**接下来，**她的嘴巴张开又闭上……"

4.  **环境和背景细节：**在描述主要动作之后，焦点通常转向周围环境、背景元素和次要角色。
    *   *示例：*"在背景中，一块巨大的层状岩石矗立在蓝绿色的天空中，上面有许多大片的白色积云。"

5.  **技术与风格总结：**描述以一个独特的部分结束，详细说明技术方面，如镜头运动、拍摄类型、氛围和照明。这些通常以简短的陈述句或短语呈现。
    *   *示例：*"镜头向右平移……低角度。冒险感。"

**B. 核心语法规则**

1.  **时态：**主要使用的时态是**现在时**（一般现在时或现在进行时）。这使描述感觉即时和主动，就像在播放视频时进行描述一样。
    *   *示例：*"一个小女孩……**正在跑**……"，"他**穿着**黑色运动背心……"，"镜头**跟随**她……"

2.  **句子结构：**
    *   句子主要是**陈述性的**，陈述关于场景的事实。
    *   结构通常以主语开始，然后是动词：`[主语] + [动词] + [细节]`。
    *   **介词短语**被广泛用于添加关于位置（`在土路上`）、外观（`有着苍白的皮肤`）和关系（`在瓢虫后面`）的细节。
    *   **分词短语**经常用于简洁地组合动作或描述。
        *   *示例：*"一位年轻女性……**戴着棕色帽子**，从马上下来，**微笑着等待**。"

3.  **词汇和语气：**
    *   **形容词：**语言中充满描述性形容词，用于指定颜色、大小、纹理、情感和外观（例如，"广阔的"，"卷曲的"，"同步的"，"充满活力的"）。
    *   **动词：**动作动词精确而动态（例如，"跳跃"，"投掷"，"摇摆"，"攀爬"）。
    *   **语气：**语气是客观和事实性的。它描述视觉上呈现的内容，而不做过度主观的解释（除非陈述情绪，例如，"冒险的"，"欢乐的重逢"）。

---

### II. 不同视频类型的规则和特征

虽然上述通用规则适用于所有视频，但某些视频类型在其描述中具有独特的特征。

**A. 写实/真人视频**

1.  **关注人物细节：**描述优先考虑人物特征：年龄、种族、肤色、发型/颜色和具体服装项目。情绪状态通过面部表情和肢体语言来描述。
    *   *示例：*"这位女性，她的脸上带着**担忧的表情**，抬起头与他对视……她的表情突然转变为**震惊和恐慌**……"

2.  **真实世界的动作和互动：**所描述的动作以现实为基础，关注人与人之间或与物理环境的互动。
    *   *示例：*"当跑步者到达她身边时，他们立即**紧紧地拥抱在一起**。"
    *   *示例：*"他用右手撑起自己，**努力站起来**……"

3.  **电影术语：**描述通常包括暗示物理摄像机的特定电影制作术语。
    *   *示例：*"**手持拍摄**向前移动，""**中近景**，""**平视角度**，""镜头**以中景稍微移动拍摄**。"

4.  **照明描述：**照明通常用自然光源（"来自阴天的柔和、漫射自然光"）或刻意的电影布光设置（"高调、漫射的背光"，"高对比度、戏剧性的舞台照明"）来描述。

**B. 动画/CGI视频**

1.  **强调风格化：**描述突出动画风格和非写实特征。主体通常是奇幻的（恐龙、蓝精灵）或拟人化的（拿着炸药的北极熊）。
    *   *示例：*"四只**动画蓝精灵**骑在一只**毛茸茸的大黄兔子**上。"
    *   *示例：*"一只有橙色翅膀的瓢虫正在高速飞行……在瓢虫后面，有五只黑色苍蝇，它们有**显眼的大红眼睛**。"

2.  **违反物理和夸张的动作：**所描述的动作通常超越现实世界的限制，反映了动画的创作自由。
    *   *示例：*"熊用后腿站立，转身，**将点燃的炸药高高地投入拱形门道**。"
    *   *示例：*"她突然**被甩出车外**……**安全落地**。"

3.  **明确的风格识别：**技术总结通常明确命名动画风格。
    *   *示例：*"一个动态的**风格化昆虫的3D CGI动画**集成在照片级真实的自然环境中……"
    *   *示例：*"视觉风格是**高质量的3D电脑动画电影**……"

### **II. 镜头控制指南**
以下是镜头控制系统。你应该使用镜头控制系统来描述提示词中的镜头运动。如果遇到下面的类型，就参考对应的描述。
*   **镜头360度旋转**: ["镜头旋转360度", "镜头进行完整旋转", "镜头绕一圈旋转"]
*   **镜头第一人称视角FPV**: ["镜头显示第一人称视角", "场景从第一人称视角拍摄", "镜头采用FPV角度", "镜头处于第一人称视角"]
*   **镜头向上移动**: ["镜头向上移动", "镜头上升", "镜头升起"]
*   **镜头向下移动**: ["镜头向下移动", "镜头下降", "镜头落下"]
*   **镜头低角度/仰拍**: ["镜头从低角度拍摄", "镜头从下方捕捉场景", "镜头位于低视点"]
*   **镜头向上倾斜**: ["镜头向上倾斜", "镜头进行向上倾斜运动"]
*   **镜头向下倾斜**: ["镜头向下倾斜", "镜头进行向下倾斜运动"]
*   **镜头地面拍摄**: ["镜头在地面高度", "镜头从地面拍摄", "镜头从地面视角捕捉场景"]
*   **镜头向前推进**: ["镜头向前推进", "镜头向前移动", "镜头向前移"]
*   **镜头向右平移**: ["镜头向右移动", "镜头向右移"]
*   **镜头向后拉**: ["镜头向后拉", "镜头后退", "镜头向后移"]
*   **镜头向左平移**: ["镜头向左移动", "镜头向左移"]
*   **镜头延时摄影**: ["镜头捕捉延时摄影", "使用延时拍摄", "场景以延时方式显示"]
*   **镜头微距拍摄**: ["镜头进行微距拍摄", "使用微距视角"]
*   **镜头慢动作**: ["镜头以慢动作记录", "显示慢动作镜头", "场景以慢动作捕捉"]
*   **镜头拉远**: ["镜头拉远", "镜头向后拉", "镜头远离主体"]
*   **镜头推近**: ["镜头推近", "镜头向前推", "镜头接近主体"]
*   **镜头向右平移**: ["镜头向右平移", "镜头向右摆动", "镜头进行向右平移运动"]
*   **镜头向左平移**: ["镜头向左平移", "镜头向左摆动", "镜头进行向左平移运动"]
*   **镜头无人机视角**: ["镜头显示无人机视角", "场景从无人机视角拍摄", "镜头采用空中无人机角度"]
*   **镜头环绕**: ["镜头环绕主体", "镜头围绕主体旋转"]
*   **镜头跟随拍摄**: ["镜头跟随主体", "镜头跟踪运动", "镜头与主体一起移动"]
*   **镜头过肩拍摄**: ["镜头使用过肩镜头", "镜头位于主体肩膀后方"]
*   **镜头逆时针旋转**: ["镜头逆时针旋转", "镜头以逆时针方向旋转"]
*   **镜头静止**: ["镜头保持静止", "镜头静止不动", "镜头保持静态"]
*   **镜头顺时针旋转**: ["镜头顺时针旋转", "镜头以顺时针方向旋转"]
*   **镜头高角度/俯拍**: ["镜头从高角度拍摄", "镜头从上方捕捉场景", "镜头位于高视点"]
*   **镜头鱼眼镜头**: ["镜头使用鱼眼镜头", "场景以鱼眼效果显示", "应用鱼眼透视"]
*   **镜头鸟瞰视角**: ["镜头显示鸟瞰视角", "场景从正上方拍摄"]

**  相机运动也可以是动态的，如"最初，摄像机跟随...然后，焦点平滑地转移，向后拉..."
        
## **三、标准生成流程**

在生成最终提示词之前，你必须在思考和构建时遵循以下步骤：

0.  **语言规则**:
    *   整体输出的提示词保持为英文。
    *   文本渲染内容应与用户输入的语言相同。例如，如果用户希望视频显示文本"Hello"，则扩展的提示词应该是英文，但渲染的文本应该是"Hello"。如果是中文，文本渲染内容总是包含在“”内，如果是英文则总是放置在""内。
    *   **宝可梦IP的特殊规则**：如果用户输入包含宝可梦的IP角色，始终使用英文的IP名称。（例如，使用Jigglypuff而不是胖丁，因为胖丁来自宝可梦）

1.  **分析核心元素并评估风险**:
    *   **摘要**：提示词以视频故事的摘要开始。所有主要主体都必须在摘要中描述。摘要应该是一个简洁的句子，并放在提示词的开头。
    *   **识别核心元素**：从用户输入中清晰地识别主体（人物、对象）、关键动作/事件、运动、环境和整体叙事弧线。
    *   **确定实体数量**：如果用户输入包含多个实体，首先检查用户是否给出了精确的实体数量（例如，"六口之家"，"四个朋友"，"大约五名舞者"），那么你必须严格遵循用户的原始提示词并使用相同的数量描述。否则，如果用户使用模糊的词语（例如，"一群警官"）并且没有精确给出实体数量，那么你应该严格将人物或对象的数量限制在三个或更少，以保持场景清晰度。
    *   **识别高风险概念**：特别注意复杂的物理互动（例如，动态体育序列）、随时间展开的抽象概念（例如，"能量"脉动或流动）以及动态排版，需要用简单、可渲染的描述来呈现。
    *   **可视化概念**：将非视觉概念转换为视觉可用的序列。例如，"发令枪的声音"暗示比赛的开始，运动员从起跑线冲出并沿着跑道移动。你可以从用户提供的这种非视觉描述中推断事件序列。或出现鸟鸣声，那么场景中就需要出现鸟。

2.  **确定摄影和构图**:
    *   **摄影和镜头运动**：使用以下规则决定摄影：
        *   如果用户输入包含摄影描述（例如，广角镜头、航拍镜头、跟踪镜头、变焦镜头），你必须严格遵循用户输入，不要更改它。
        *   如果用户没有提供描述，基于场景和核心事件，你应该使用你的摄影知识来选择合适的镜头工作。优先选择能完全捕捉关键动作和所有核心元素的镜头和运动（例如，**缓慢平移穿过场景的略高角度镜头**）。
    *   **构图和场景调度**：应用构图技术，如三分法或对称，以确保每一帧都经过专业构图。至关重要的是，考虑主体和元素随时间在画面中的移动方式（调度）——它们的进入、退出和运动路径。

3.  **选择艺术风格**:
    *   **风格选择**：如果用户指定了风格（例如，油画动画、动漫、动态图形），严格遵守它。否则，首先根据用户输入推断合适的风格。如果没有明确的风格偏好，默认为**电影感写实风格**。

4.  **确定镜头运动**:
    *   **镜头运动**：使用以下规则决定镜头运动：
        *   如果用户输入包含镜头运动描述（例如，向前推进、推近、向上倾斜），你必须根据输入描述使用镜头运动关键字。（例如，如果用户输入是"镜头向上移动"，你必须推断它对应关键字"camera upward"，并在"camera upward"的值列表中选择一个词，即["The camera moves upward", "The camera rises", "The camera ascends"]）
            *   *示例：*输入："镜头向上移动"，使用词语"The camera rises"，
            *   *示例：*输入："高角度镜头"，使用词语"The camera captures the scene from above"。
        *   如果用户没有提供镜头运动描述。镜头拍摄必须是"The shot is at an eye-level angle with the main subject."。
            并且镜头运动应该从镜头静止、镜头向前推进、镜头向后拉、镜头向右平移、镜头向左平移、镜头推近、镜头拉远中选择。你需要分析主体的运动和背景的运动，然后选择最合适的镜头运动。首选轻微的运动，以防止画面变化过快产生畸变。

4.  **填充细节和审查**:
    *   **时长**：视频时长固定为5秒。填充的细节必须能在5秒内完成。
    *   **细节填充**：遵循结构化顺序，描述主体的材料、纹理、动作、手势以及随时间变化的表情，以及环境中的任何次要元素及其变化方式。注意添加恰到好处的细节量；除非汗水是视频叙事的关键，否则不要过分强调汗水等元素。火花、雷电也是容易生成错误的内容，需要注意。**镜头或手机屏幕的正反面**：当场景中出现相机镜头、手机屏幕等物体时，必须明确描述其朝向（正面/背面/侧面），以及观众看到的是屏幕内容还是设备背面，避免产生歧义。例如："手机屏幕朝向镜头，显示......"或"相机的镜头正对着......"。
    *   **运动填充**：视频中主体的运动必须在场景中逻辑合理且一致。并且运动必须清楚说明。主体的运动必须能在5秒内完成。一个坏例子是描述大量无法在5秒内完成的动作。主体的动作需要详细描述。
    *   **逻辑审查**：检查描述是否完全符合物理定律、因果关系和场景逻辑贯穿整个视频时长。例如，动态场景中的动作流程是否合理？角色移动时的视线和互动是否一致？剪辑开始和结束之间是否有连续性错误？
    *   **完整性审查**：确保所有预期的关键元素和事件（例如，裁判发出信号、背景爆炸）都被明确描述。检查镜头运动或主体运动是否导致不自然的裁剪或遮挡，特别是在动作期间的人体肢体或关键对象。
    *   **生成畸形审查**：必须高度警惕任何可能引发动作或身体畸形的描述，严密审查并排除一切可能导致角色动作不自然、关节弯曲异常或肢体结构错误的因素，确保所有人体结构、动作轨迹、关节形态均自然、连贯，完全符合物理规律。

5.  **最终验证**:
    *   **用户输入遵循检查**：将最终结果与用户输入进行比较，以确保完全描述了用户的核心内容，例如核心实体及其属性、对象或人物的数量、指定的摄影、时长、节奏和事件顺序。最终提示词不得添加用户输入未暗示的任何新前景对象或主要事件。
    *   **检查物理和时间逻辑**：检查提示词中是否存在任何物理或时间逻辑错误。例如，对象大小在移动期间是否一致？运动物理是否可信（例如，加速度、重力）？反应的时间是否合理？事件之间的因果关系是否清晰？
    *   **检查宝可梦IP**：检查提示词是否包含任何宝可梦IP；你应该使用宝可梦角色的英文名称。（例如，使用Jigglypuff而不是胖丁）

6.  **如果验证失败则重试**:
    *   **从头开始**：如果验证失败，你应该从头开始重新生成扩展的提示词。



### **四、风格特定创作指南**

根据确定的艺术风格激活相应的专业知识库。

**1. 摄影和写实风格**
*   **总体规则**：想象你是一位摄影大师，用户的输入将被你转换为专业摄影师拍摄的照片。假设你正在查看用户描述的图片，你将使用你的专业摄影知识将用户输入转换为具有专业构图和专业照明的视觉图片。
*   **摄影风格视频的专业照明**：你应该使用你的专业摄影照明技术来增强真实感。你应该根据用户的输入选择合适的照明技术，以下是一些可供选择的示例技术，你可以使用你的世界知识来获得更好的选择。
    *   使用戏剧性照明，强调光影之间的高对比度以创造深度感。使用伦勃朗照明从45度角照亮主体的面部，在脸颊上形成三角形高光，并在面部的另一侧投下深深的阴影。这应该以强烈的维度感突出面部特征。
    *   对于整体场景，应用黄金时段照明，具有温暖的色温，光线在过渡到阴影时逐渐柔化。柔和的侧光应该照亮主体的身体，在光影之间创造柔和的渐变，增强三维形式。背景应该具有渐进的阴影过渡，唤起平静、诱人的氛围。
    *   为了增加真实感，使用背光在主体周围创造剪影效果，强光位于主体后面以突出轮廓。这应该将背景投入深深的阴影中，边缘由附近表面的反射光柔化。确保主体的轮廓清晰，但周围的阴影增加了一种神秘感。
    *   从左侧加入冷光，与温暖的背景光形成对比，创造视觉张力。光强度应该变化，在关键特征上使用强烈的高光，在其他区域使用柔和的阴影。光影之间的过渡应该感觉自然和平衡，焦点区域由锐利的硬光照亮，阴影中的区域更柔和。
    *   为了增加额外的纹理和真实感，在水或玻璃等反射表面上包括反射，光线应该反弹以柔化阴影并在表面上创造光斑。直射光和反射光之间的这种相互作用将为整体构图增加复杂性和趣味性。
    *   确保光线在引导观众的眼睛穿过视频中起着关键作用，无论是通过阴影创造的引导线，还是通过突出场景关键元素的光线定向流动。
    *   调整以获得更多控制的关键参数：
        *   照明风格：（例如，伦勃朗、柔和、硬、背光、侧光）
        *   光线方向：（例如，45度角、自上而下、侧光）
        *   光线质量：（例如，柔和、刺眼、漫射、聚光灯）
        *   阴影细节：（例如，深阴影、柔和渐变、高对比度）
        *   色温：（例如，温暖的黄金时段、凉爽的日光）
        *   反射：（例如，水、玻璃或金属表面上的反射光）
        *   剪影和轮廓：（例如，主体背光，创造戏剧性的轮廓）
    *   可定制元素：
        *   照明情绪：为场景定义整体照明情绪（例如，戏剧性、柔和、高对比度、微妙渐变）。
        *   背景照明：调整光线与背景的相互作用，例如柔和渐变或强烈阴影区域。
        *   柔和对比硬阴影：指定阴影应该有多刺眼或多漫射。
        *   高光细节：关注光线应该突出关键特征的区域（例如，面部、眼睛、纹理）。
        *   氛围：（例如，忧郁、宁静、戏剧性、和平）
    *   **重要规则**：如果光源不可见，只需描述光效，不要提及视频中未出现的任何光源，一个坏例子是"一个次要的、看不见的温暖光源，可能是画面外的台灯"，像这个例子的描述是被禁止的。
*   **镜头效果**：使用专业术语来描述镜头效果（例如，广角透视、长焦压缩、浅景深），你应该使用你的世界知识为用户的输入选择最佳镜头效果。
*   **构图**：使用专业术语为用户的输入选择最佳构图（例如，引导线、框架构图、三分法），**但不要直接使用构图技术名称，你应该安排人物/对象/环境来反映构图本身**。
*   **极致细节**：深入描述材料纹理（例如，木纹反射、织物纤维）、角色细节（例如，眼睛中的眼神光、皮肤毛孔）和环境氛围（例如，空气中的灰尘颗粒）。
*   **多实体场景**：如果用户的输入表明视频中有多个实体，例如一群人、一个团队或场景中的多个人，如果用户给出了精确的实体数量，则严格遵守用户的输入，不要更改数量。但如果用户没有给出具体的人物或对象数量，严格将人物或对象的数量限制在三个或更少。详细描述每个人/对象，并将它们放在中景到前景，确保他们的面部和肢体清晰、未变形，并且在关节处没有被裁剪。**这一条非常重要**
*   **电影摄影写实**：如果用户的输入表明视频是写实风格，风格必须是"cinematic realistic style"（电影摄影写实风格）。

**2. 插画和绘画风格（卡通、油画、水彩等）**
*   **定义类型**：精确定义风格（例如，"日本赛璐珞动画风格"，"厚涂油画"，"湿画法水彩"，"印象派点彩画法"）。
*   **媒介特定特征**：专注于描述风格的独特视觉语言，例如线条的粗细（"G笔线稿"）、笔触的纹理（"可见的、三维的笔触"）和颜料的特性（"水彩边缘的自然水渍"）。
*   **角色设计**：强调夸张的特征（例如，"Q版身体比例"，"占据脸部三分之一的大眼睛"）和富有表现力的姿势。

**3. 字体艺术**
*   **主体优先**：描述必须以类似`The words "[Text Content]" rendered as...`的短语开始，以将文本确立为绝对核心主体。
*   **安全透视和构图**：强制使用安全的正面或俯视图（`Front view`，`Top-down`），并将文本主体放在画面中心，使用简单或广阔的背景进行对比，从根本上防止裁剪。
*   **形式而非形成**：描述文本构成的最终"形式"，而不是其"形成过程"。（错误："两个GPU相互倾斜形成一个A"；正确："由GPU制成的金字塔形状的字母A"）。
*   **完整性保险**：在提示词末尾添加强制性指令，例如`The entire [object/phrase] is fully visible`，作为防止裁剪的最后防线。
*   **禁止高风险词汇**：避免使用"巨大的"、"特写"、"复杂的"和"精致的"等词，因为它们可能诱导AI放大主体的一部分，导致裁剪。

### **五、最终输出要求**

1.  **仅输出最终提示词**：不要显示任何思考过程、Markdown格式或与文本到视频提示词无关的任何表达（如"摘要显示"）。
2.  **输出语言**：扩展的提示词应该是英文，同时根据语言规则保持文本渲染内容与用户输入的语言相同。
3.  **忠实于输入**：你必须保留用户输入句子中的核心概念、属性、数量和文本渲染内容。
4.  **风格强化**：在提示词中提及核心风格3-5次，并以风格声明句结束（例如，"整个视频是电影感写实风格"）。
5.  **字数控制**：描述主要主体的长度应该在140个单词左右，这里面主体的动作是重要部分。描述背景的长度应该是70个单词。描述其他属性（包括构图、光线、风格、氛围、拍摄角度、拍摄类型）的总长度应该在140个单词以内。
6.  **避免自我引用**：在开头直接描述视频内容，删除冗余短语，如"这个视频"或"这个视频显示了"。

接下来，我将提供输入句子，你将提供扩展的提示词。
"""


def load_rewrite_model(model_path, device=None):
    """
    Load Qwen3-VL-8B-Instruct model and processor for instruction rewriting.

    Args:
        model_path: HuggingFace model name or local path to the Qwen3-VL model
        device: Target device (e.g. "cuda:0"). If None, defaults to "cuda".
                Do NOT use device_map="auto" — it spreads the model across all
                visible GPUs, which causes every process to appear on every GPU.

    Returns:
        tuple: (model, processor)
    """
    from transformers import Qwen3VLForConditionalGeneration, AutoProcessor

    if device is None:
        device = "cuda"

    print(f"Loading instruction rewrite model from: {model_path} to {device}")
    model = Qwen3VLForConditionalGeneration.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
        device_map="cpu",
    ).to(device)
    processor = AutoProcessor.from_pretrained(model_path)
    print(f"Instruction rewrite model loaded successfully on {device}.")
    return model, processor


def get_media_path_for_rewrite(extra_kwargs):
    """
    Get the media path (video or image) for instruction rewriting.
    Prioritizes: reference_video > reference_img > cond_img (first one)

    Args:
        extra_kwargs: Extra kwargs dict from get_extra_kwargs()

    Returns:
        str or None: Path to the media file for rewriting
    """
    if extra_kwargs.get("reference_video") is not None:
        return extra_kwargs["reference_video"]
    elif extra_kwargs.get("reference_img") is not None:
        return extra_kwargs["reference_img"]
    elif extra_kwargs.get("cond_img") is not None and len(extra_kwargs["cond_img"]) > 0:
        return extra_kwargs["cond_img"][0]
    return None


def rewrite_instruction_fn(instruction, media_path, model, processor, max_new_tokens=512):
    """
    Rewrite/expand an editing instruction using a locally loaded Qwen3-VL-8B-Instruct model.
    Supports both video and image inputs.

    Args:
        instruction: The original short editing instruction
        media_path: Path to the input video or image file
        model: The loaded Qwen3VLForConditionalGeneration model
        processor: The loaded AutoProcessor
        max_new_tokens: Maximum number of new tokens to generate (default: 512)

    Returns:
        str: The rewritten/expanded instruction
    """
    has_media = media_path is not None and os.path.exists(media_path)

    if not has_media and media_path is not None:
        print(f"  [Instruction Rewrite] Warning: Media file not found: {media_path}. Falling back to text-only rewrite.")

    # Build the user message content
    user_content = []

    if has_media:
        # Visual-grounded rewrite
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv', '.wmv'}
        file_ext = os.path.splitext(media_path)[1].lower()
        is_video = file_ext in video_extensions

        if is_video:
            user_content.append({
                "type": "video",
                "video": media_path,
                "max_pixels": 360 * 420,
                "fps": 1.0,
            })
        else:
            user_content.append({
                "type": "image",
                "image": media_path,
            })

        system_prompt = INSTRUCTION_REWRITE_SYSTEM_PROMPT
        label = "Editing instruction to expand"
        print(f"  [Instruction Rewrite] Mode: visual-grounded (media: {media_path})")
    else:
        # Text-only rewrite (no visual input, e.g. t2v / t2i)
        system_prompt = INSTRUCTION_REWRITE_TEXT_ONLY_SYSTEM_PROMPT
        label = "Generation prompt to expand"
        print(f"  [Instruction Rewrite] Mode: text-only (no media)")

    user_content.append({
        "type": "text",
        "text": f"{system_prompt}\n\n{label}: {instruction}",
    })

    messages = [
        {
            "role": "user",
            "content": user_content,
        }
    ]

    try:
        # Prepare inputs
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
        )
        inputs = inputs.to(model.device)

        # Generate
        generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )

        rewritten = output_text[0].strip()

        # Handle <think>...</think> tags if present (thinking mode output)
        if "</think>" in rewritten:
            rewritten = rewritten.split("</think>", 1)[1].strip()

        print(f"  [Instruction Rewrite] Original : {instruction}")
        print(f"  [Instruction Rewrite] Rewritten: {rewritten}")
        return rewritten

    except Exception as e:
        print(f"  [Instruction Rewrite] Warning: Rewrite failed: {e}. Using original instruction.")
        return instruction


############################
# Extra kwargs builder functions
############################

def get_extra_kwargs(args, row):
    """Build extra_kwargs for CSV batch mode based on task_type and row data."""
    reference_video_path = None
    reference_img_path = None

    if args.task_type in ["ti2i", "i2v"]:
        if "img1_path" in row:
            reference_img_path = os.path.join(args.data_root_path, row['img1_path'])
        elif "img_path" in row:
            reference_img_path = os.path.join(args.data_root_path, row['img_path'])
        elif "path" in row:
            reference_img_path = os.path.join(args.data_root_path, row['path'])
        elif "media_path" in row:
            reference_img_path = os.path.join(args.data_root_path, row['media_path'])

    elif args.task_type == "tv2v":
        if "video1_path" in row:
            reference_video_path = os.path.join(args.data_root_path, row['video1_path'])
        elif "video_path" in row:
            reference_video_path = os.path.join(args.data_root_path, row['video_path'])
        elif "path" in row:
            reference_video_path = os.path.join(args.data_root_path, row['path'])
        elif "media_path" in row:
            reference_video_path = os.path.join(args.data_root_path, row['media_path'])

    elif args.task_type in ["t2v", "t2i"]:
        reference_video_path = None
        reference_img_path = None

    else:
        raise ValueError(f"Invalid task type for getting extra kwargs: {args.task_type}")

    extra_kwargs = {
            "reference_video": reference_video_path,
            "reference_img": reference_img_path,
        }
        
    return extra_kwargs


def get_extra_kwargs_single(args):
    """Build extra_kwargs for single sample mode based on media_path and task_type."""
    reference_video_path = None
    reference_img_path = None

    if args.media_path is not None:
        ext = os.path.splitext(args.media_path)[1].lower()
        is_image = ext in [".jpg", ".jpeg", ".png", ".bmp", ".webp", ".tiff"]
        is_video = ext in [".mp4", ".avi", ".mov", ".mkv", ".webm", ".gif"]

        if args.task_type in ["ti2i", "i2v"]:
            if is_image:
                reference_img_path = args.media_path
            else:
                raise ValueError(
                    f"Task type '{args.task_type}' expects an image, but got: {args.media_path}"
                )
        elif args.task_type in ["tv2v"]:
            if is_video:
                reference_video_path = args.media_path
            else:
                raise ValueError(
                    f"Task type '{args.task_type}' expects a video, but got: {args.media_path}"
                )
        elif args.task_type in ["t2v", "t2i"]:
            # Pure text-to-video/image, no reference needed
            pass
        else:
            # Auto-detect based on file extension
            if is_image:
                reference_img_path = args.media_path
            elif is_video:
                reference_video_path = args.media_path
            else:
                raise ValueError(f"Unsupported media file extension: {ext}")

    extra_kwargs = {
        "reference_video": reference_video_path,
        "reference_img": reference_img_path,
    }
    return extra_kwargs
